{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9717889,"sourceType":"datasetVersion","datasetId":5945249},{"sourceId":9718194,"sourceType":"datasetVersion","datasetId":5945453}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of Contents\n1. [Setting Up](#setting-up)\n2. [Loading the dataset](#loading-the-dataset)\n3. [Preprocessing](#preprocessing)\n4. [Preparing dataset for training and validating](#preparing-dataset-for-training-and-validating)\n5. [Model Training and Evaluation](#model-training-and-evaluation)\n6. [Comparing models](#comparing-models)\n7. [Parameter tuning for Logistic Regression](#parameter-tuning-for-logistic-regression)\n8. [Getting and evaluating the optimized logistic regression model](#getting-and-evaluating-the-optimized-logistic-regression-model)\n9. [Saving the model](#saving-the-model)","metadata":{}},{"cell_type":"markdown","source":"## Setting Up","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.pipeline import Pipeline\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:22.325665Z","iopub.execute_input":"2024-10-25T16:28:22.326103Z","iopub.status.idle":"2024-10-25T16:28:24.134133Z","shell.execute_reply.started":"2024-10-25T16:28:22.326045Z","shell.execute_reply":"2024-10-25T16:28:24.132865Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"# Load the CSV file\nfile_path = '/kaggle/input/train-data/credit_card_train.csv'\ndf = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:24.137389Z","iopub.execute_input":"2024-10-25T16:28:24.138219Z","iopub.status.idle":"2024-10-25T16:28:24.740738Z","shell.execute_reply.started":"2024-10-25T16:28:24.138156Z","shell.execute_reply":"2024-10-25T16:28:24.739355Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:24.742430Z","iopub.execute_input":"2024-10-25T16:28:24.742901Z","iopub.status.idle":"2024-10-25T16:28:24.778615Z","shell.execute_reply.started":"2024-10-25T16:28:24.742857Z","shell.execute_reply":"2024-10-25T16:28:24.777055Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        Num_Children  Gender  Income Own_Car Own_Housing Credit_Card_Issuing\n0                  1    Male   40690      No         Yes              Denied\n1                  2  Female   75469     Yes          No              Denied\n2                  1    Male   70497     Yes         Yes            Approved\n3                  1    Male   61000      No          No              Denied\n4                  1    Male   56666     Yes         Yes              Denied\n...              ...     ...     ...     ...         ...                 ...\n399995             2    Male   55332     Yes          No              Denied\n399996             1    Male   95108      No          No            Approved\n399997             3    Male   57163     Yes         Yes              Denied\n399998             5    Male  112237     Yes         Yes            Approved\n399999             2    Male   62439     Yes         Yes              Denied\n\n[400000 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Num_Children</th>\n      <th>Gender</th>\n      <th>Income</th>\n      <th>Own_Car</th>\n      <th>Own_Housing</th>\n      <th>Credit_Card_Issuing</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Male</td>\n      <td>40690</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Denied</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Female</td>\n      <td>75469</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Denied</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Male</td>\n      <td>70497</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Approved</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Male</td>\n      <td>61000</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Denied</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Male</td>\n      <td>56666</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Denied</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>399995</th>\n      <td>2</td>\n      <td>Male</td>\n      <td>55332</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Denied</td>\n    </tr>\n    <tr>\n      <th>399996</th>\n      <td>1</td>\n      <td>Male</td>\n      <td>95108</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Approved</td>\n    </tr>\n    <tr>\n      <th>399997</th>\n      <td>3</td>\n      <td>Male</td>\n      <td>57163</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Denied</td>\n    </tr>\n    <tr>\n      <th>399998</th>\n      <td>5</td>\n      <td>Male</td>\n      <td>112237</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Approved</td>\n    </tr>\n    <tr>\n      <th>399999</th>\n      <td>2</td>\n      <td>Male</td>\n      <td>62439</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Denied</td>\n    </tr>\n  </tbody>\n</table>\n<p>400000 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# Convert the target variable 'Credit_Card_Issuing' to binary (1 for Approved, 0 for Denied)\ndf['Credit_Card_Issuing'] = df['Credit_Card_Issuing'].apply(lambda x: 1 if x == 'Approved' else 0)\n\n# Label encode categorical variables\nle = LabelEncoder()\ndf['Gender'] = le.fit_transform(df['Gender'])\ndf['Own_Car'] = le.fit_transform(df['Own_Car'])\ndf['Own_Housing'] = le.fit_transform(df['Own_Housing'])","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:24.782337Z","iopub.execute_input":"2024-10-25T16:28:24.782952Z","iopub.status.idle":"2024-10-25T16:28:25.456775Z","shell.execute_reply.started":"2024-10-25T16:28:24.782904Z","shell.execute_reply":"2024-10-25T16:28:25.455531Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:25.458912Z","iopub.execute_input":"2024-10-25T16:28:25.459445Z","iopub.status.idle":"2024-10-25T16:28:25.477897Z","shell.execute_reply.started":"2024-10-25T16:28:25.459382Z","shell.execute_reply":"2024-10-25T16:28:25.476538Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        Num_Children  Gender  Income  Own_Car  Own_Housing  \\\n0                  1       1   40690        0            1   \n1                  2       0   75469        1            0   \n2                  1       1   70497        1            1   \n3                  1       1   61000        0            0   \n4                  1       1   56666        1            1   \n...              ...     ...     ...      ...          ...   \n399995             2       1   55332        1            0   \n399996             1       1   95108        0            0   \n399997             3       1   57163        1            1   \n399998             5       1  112237        1            1   \n399999             2       1   62439        1            1   \n\n        Credit_Card_Issuing  \n0                         0  \n1                         0  \n2                         1  \n3                         0  \n4                         0  \n...                     ...  \n399995                    0  \n399996                    1  \n399997                    0  \n399998                    1  \n399999                    0  \n\n[400000 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Num_Children</th>\n      <th>Gender</th>\n      <th>Income</th>\n      <th>Own_Car</th>\n      <th>Own_Housing</th>\n      <th>Credit_Card_Issuing</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>40690</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>75469</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>70497</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>61000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>56666</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>399995</th>\n      <td>2</td>\n      <td>1</td>\n      <td>55332</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>399996</th>\n      <td>1</td>\n      <td>1</td>\n      <td>95108</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>399997</th>\n      <td>3</td>\n      <td>1</td>\n      <td>57163</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>399998</th>\n      <td>5</td>\n      <td>1</td>\n      <td>112237</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>399999</th>\n      <td>2</td>\n      <td>1</td>\n      <td>62439</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>400000 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preparing dataset for training and validating","metadata":{}},{"cell_type":"code","source":"# Separate features and target\nX = df.drop('Credit_Card_Issuing', axis=1)\ny = df['Credit_Card_Issuing']\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:25.479570Z","iopub.execute_input":"2024-10-25T16:28:25.479933Z","iopub.status.idle":"2024-10-25T16:28:25.560791Z","shell.execute_reply.started":"2024-10-25T16:28:25.479893Z","shell.execute_reply":"2024-10-25T16:28:25.559640Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"# Check for gender-based bias\nmale_indices = X_test['Gender'] == 1\nfemale_indices = X_test['Gender'] == 0","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:25.562116Z","iopub.execute_input":"2024-10-25T16:28:25.562478Z","iopub.status.idle":"2024-10-25T16:28:25.569768Z","shell.execute_reply.started":"2024-10-25T16:28:25.562420Z","shell.execute_reply":"2024-10-25T16:28:25.568255Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Function to train, predict, evaluate models, and optionally return the model\ndef train_and_evaluate_model(model, model_name, return_model=False):\n    # Create a pipeline with the model\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', model)\n    ])\n\n    # Train the model\n    pipeline.fit(X_train, y_train)\n\n    # Predict on the test set\n    y_pred = pipeline.predict(X_test)\n\n    # Performance metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n\n    print(f\"Performance for {model_name}:\")\n    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision * 100:.2f}%\")\n    print(f\"Recall: {recall * 100:.2f}%\")\n    print(f\"F1-Score: {f1 * 100:.2f}%\")\n\n    # Fairness: Check classification reports for males and females\n    y_pred_male = pipeline.predict(X_test[male_indices])\n    y_true_male = y_test[male_indices]\n    y_pred_female = pipeline.predict(X_test[female_indices])\n    y_true_female = y_test[female_indices]\n\n    print(\"\\nBias/Fairness Evaluation:\")\n    print(f\"Male Classification Report for {model_name}:\")\n    print(classification_report(y_true_male, y_pred_male))\n    print(f\"Female Classification Report for {model_name}:\")\n    print(classification_report(y_true_female, y_pred_female))\n\n    # Variance: Compare training and test performance\n    y_train_pred = pipeline.predict(X_train)\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    print(f\"\\nVariance Check for {model_name}:\")\n    print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n    \n    # Interpretability: Feature importance for Logistic Regression and Random Forest\n    if hasattr(pipeline.named_steps['model'], 'coef_'):  # For Logistic Regression\n        feature_importance = pipeline.named_steps['model'].coef_[0]\n        feature_names = X.columns\n        importance_dict = dict(zip(feature_names, feature_importance))\n        importance_sorted = sorted(importance_dict.items(), key=lambda item: abs(item[1]), reverse=True)\n        print(\"\\nFeature Importance for Logistic Regression:\")\n        for feature, importance in importance_sorted:\n            print(f\"{feature}: {importance:.2f}\")\n    elif hasattr(pipeline.named_steps['model'], 'feature_importances_'):  # For Random Forest and XGBoost\n        feature_importance = pipeline.named_steps['model'].feature_importances_\n        feature_names = X.columns\n        importance_dict = dict(zip(feature_names, feature_importance))\n        importance_sorted = sorted(importance_dict.items(), key=lambda item: abs(item[1]), reverse=True)\n        print(f\"\\nFeature Importance for {model_name}:\")\n        for feature, importance in importance_sorted:\n            print(f\"{feature}: {importance:.2f}\")\n\n    # Optionally return the trained model\n    if return_model:\n        return pipeline","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:25.571923Z","iopub.execute_input":"2024-10-25T16:28:25.572357Z","iopub.status.idle":"2024-10-25T16:28:25.590684Z","shell.execute_reply.started":"2024-10-25T16:28:25.572316Z","shell.execute_reply":"2024-10-25T16:28:25.589377Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Comments","metadata":{}},{"cell_type":"markdown","source":"- **Standardization (Scaler)**: The data is scaled using `StandardScaler` to ensure that all features have a similar scale, which is crucial for models like Logistic Regression that are sensitive to feature magnitude.","metadata":{}},{"cell_type":"markdown","source":"1. **Performance Evaluation**:\n    - **Accuracy**: The overall accuracy of the model is calculated, indicating how many predictions the model got right out of all predictions. For instance, the accuracy achieved was 97.26%.\n\n    - **Precision**: Precision measures how many of the predicted positive cases were actually positive. This metric is especially important in cases where false positives are costly.\n\n    - **Recall**: Recall evaluates how many of the actual positive cases were correctly identified by the model. High recall ensures that most true positives are detected.\n\n    - **F1-Score**: The F1-score, which is the harmonic mean of precision and recall, is used as a balanced metric when both false positives and false negatives matter. This score was calculated for both the overall dataset and for specific groups (males and females).\n\n2. **Fairness and Bias Evaluation**: Predictions are evaluated separately for males and females by splitting the test set (`X_test`) into `male_indices` and `female_indices`. Classification reports for both genders are generated to check for bias and ensure fairness in model performance.\n\n3. **Variance Check**: Training and test accuracies are compared to check for overfitting or underfitting. In this case, both accuracies are nearly the same, which shows the model generalizes well.\n\n4. **Feature Importance**: For models like Logistic Regression, feature importance is derived from the coefficients (`coef_`). The most impactful features are identified and sorted, providing insights into which factors contributed most to the model's predictions (e.g., income, gender, etc.).","metadata":{}},{"cell_type":"markdown","source":"## Comparing models","metadata":{}},{"cell_type":"code","source":"# Models to compare\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n}\n\n# Train and evaluate all models\nfor model_name, model in models.items():\n    train_and_evaluate_model(model, model_name)\n    print(\"--------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:28:25.592265Z","iopub.execute_input":"2024-10-25T16:28:25.592662Z","iopub.status.idle":"2024-10-25T16:29:13.700920Z","shell.execute_reply.started":"2024-10-25T16:28:25.592623Z","shell.execute_reply":"2024-10-25T16:29:13.699532Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Performance for Logistic Regression:\nAccuracy: 97.25%\nPrecision: 96.50%\nRecall: 96.40%\nF1-Score: 96.45%\n\nBias/Fairness Evaluation:\nMale Classification Report for Logistic Regression:\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97     17045\n           1       0.98      0.98      0.98     22910\n\n    accuracy                           0.97     39955\n   macro avg       0.97      0.97      0.97     39955\nweighted avg       0.97      0.97      0.97     39955\n\nFemale Classification Report for Logistic Regression:\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98     32024\n           1       0.93      0.93      0.93      8021\n\n    accuracy                           0.97     40045\n   macro avg       0.96      0.95      0.95     40045\nweighted avg       0.97      0.97      0.97     40045\n\n\nVariance Check for Logistic Regression:\nTraining Accuracy: 97.31%\nTest Accuracy: 97.25%\n\nFeature Importance for Logistic Regression:\nIncome: 17.71\nGender: 3.87\nOwn_Housing: 2.66\nOwn_Car: 1.08\nNum_Children: -0.01\n--------------------------------------------------------------------\nPerformance for Random Forest:\nAccuracy: 96.48%\nPrecision: 95.51%\nRecall: 95.38%\nF1-Score: 95.45%\n\nBias/Fairness Evaluation:\nMale Classification Report for Random Forest:\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96     17045\n           1       0.97      0.97      0.97     22910\n\n    accuracy                           0.97     39955\n   macro avg       0.97      0.97      0.97     39955\nweighted avg       0.97      0.97      0.97     39955\n\nFemale Classification Report for Random Forest:\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98     32024\n           1       0.91      0.91      0.91      8021\n\n    accuracy                           0.96     40045\n   macro avg       0.94      0.94      0.94     40045\nweighted avg       0.96      0.96      0.96     40045\n\n\nVariance Check for Random Forest:\nTraining Accuracy: 99.77%\nTest Accuracy: 96.48%\n\nFeature Importance for Random Forest:\nIncome: 0.85\nGender: 0.10\nOwn_Housing: 0.03\nOwn_Car: 0.01\nNum_Children: 0.00\n--------------------------------------------------------------------\nPerformance for XGBoost:\nAccuracy: 97.24%\nPrecision: 96.61%\nRecall: 96.25%\nF1-Score: 96.43%\n\nBias/Fairness Evaluation:\nMale Classification Report for XGBoost:\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97     17045\n           1       0.98      0.98      0.98     22910\n\n    accuracy                           0.97     39955\n   macro avg       0.97      0.97      0.97     39955\nweighted avg       0.97      0.97      0.97     39955\n\nFemale Classification Report for XGBoost:\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98     32024\n           1       0.93      0.92      0.93      8021\n\n    accuracy                           0.97     40045\n   macro avg       0.96      0.95      0.96     40045\nweighted avg       0.97      0.97      0.97     40045\n\n\nVariance Check for XGBoost:\nTraining Accuracy: 97.39%\nTest Accuracy: 97.24%\n\nFeature Importance for XGBoost:\nGender: 0.47\nIncome: 0.35\nOwn_Housing: 0.16\nOwn_Car: 0.02\nNum_Children: 0.00\n--------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Comments","metadata":{}},{"cell_type":"markdown","source":"- **Best Model:** Based on accuracy, precision, recall, F1-score, and fairness evaluation, **Logistic Regression** seems to be the best-performing model. It achieves the highest balance between performance and fairness, without significant overfitting or bias across genders.\n\n- **General Comments:**\n    - Random Forest shows signs of overfitting with a large gap between training and test accuracy, making it less generalizable to unseen data.\n    - All models show relatively fair performance across genders, but Logistic Regression and XGBoost perform slightly better for female applicants than Random Forest.\n    - Across all models, Income is the most important factor in determining credit card approval. Logistic Regression gives higher importance to features like Gender and Own_Housing, while XGBoost assigns more importance to Gender.","metadata":{}},{"cell_type":"markdown","source":"## Parameter tuning for Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Parameter tuning for Logistic Regression\nparam_grid_lr = {\n    'model__C': [0.01, 0.1, 1, 10, 100],\n    'model__penalty': ['l1', 'l2'],\n    'model__solver': ['liblinear']\n}\n\n# Grid Search for Logistic Regression\ngrid_search_lr = GridSearchCV(Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression())]), \n                              param_grid_lr, cv=5, scoring='accuracy')\ngrid_search_lr.fit(X_train, y_train)\n\n# Get best params and score for Logistic Regression\nbest_params_lr = grid_search_lr.best_params_\nbest_score_lr = grid_search_lr.best_score_\n\nprint(f\"\\nBest parameters for Logistic Regression: {best_params_lr}\")\nprint(f\"Best cross-validation score: {best_score_lr * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:29:13.704213Z","iopub.execute_input":"2024-10-25T16:29:13.704615Z","iopub.status.idle":"2024-10-25T16:30:28.730744Z","shell.execute_reply.started":"2024-10-25T16:29:13.704573Z","shell.execute_reply":"2024-10-25T16:30:28.727982Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nBest parameters for Logistic Regression: {'model__C': 0.1, 'model__penalty': 'l1', 'model__solver': 'liblinear'}\nBest cross-validation score: 97.31%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Comments","metadata":{}},{"cell_type":"markdown","source":"- **Best Parameters:**\n    - Best C value (0.1): This indicates moderate regularization, striking a balance between preventing overfitting while still fitting the data well.\n    - L1 penalty: This suggests that some feature coefficients may have been shrunk to zero, simplifying the model and helping with feature selection.\n- **High cross-validation score (97.31%):** This confirms that the model is performing very well on multiple folds of the data and can generalize effectively to unseen data.","metadata":{}},{"cell_type":"markdown","source":"## Getting and evaluating the optimized logistic regression model","metadata":{}},{"cell_type":"code","source":"# Apply the best hyperparameters from GridSearchCV\nbest_logistic_regression = LogisticRegression(C=0.1, penalty='l1', solver='liblinear')\n\n# Train and evaluate the optimized Logistic Regression model\nbest_logistic_regression = train_and_evaluate_model(best_logistic_regression, \"Optimized Logistic Regression\", return_model=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:35:36.674998Z","iopub.execute_input":"2024-10-25T16:35:36.675600Z","iopub.status.idle":"2024-10-25T16:35:39.232915Z","shell.execute_reply.started":"2024-10-25T16:35:36.675554Z","shell.execute_reply":"2024-10-25T16:35:39.231756Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Performance for Optimized Logistic Regression:\nAccuracy: 97.26%\nPrecision: 96.49%\nRecall: 96.41%\nF1-Score: 96.45%\n\nBias/Fairness Evaluation:\nMale Classification Report for Optimized Logistic Regression:\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97     17045\n           1       0.98      0.98      0.98     22910\n\n    accuracy                           0.97     39955\n   macro avg       0.97      0.97      0.97     39955\nweighted avg       0.97      0.97      0.97     39955\n\nFemale Classification Report for Optimized Logistic Regression:\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98     32024\n           1       0.93      0.93      0.93      8021\n\n    accuracy                           0.97     40045\n   macro avg       0.96      0.95      0.95     40045\nweighted avg       0.97      0.97      0.97     40045\n\n\nVariance Check for Optimized Logistic Regression:\nTraining Accuracy: 97.31%\nTest Accuracy: 97.26%\n\nFeature Importance for Logistic Regression:\nIncome: 17.67\nGender: 3.86\nOwn_Housing: 2.65\nOwn_Car: 1.07\nNum_Children: -0.01\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Comments","metadata":{}},{"cell_type":"markdown","source":"- **Model Performance**:\n    - **Accuracy**: The model performs well with 97.26% accuracy, meaning most predictions are correct.\n    - **Precision**: Precision is high for both genders, especially for males (96.49%), meaning the model rarely mislabels negatives as positives.\n    - **Recall**: The model effectively identifies true positives with recall scores around 96.41%.\n    - **F1-Score**: The F1-scores are balanced and high, showing the model has a good balance between precision and recall.\n\n\n- **Bias/Fairness Evaluation**:\n    - **Male Classification**: Precision, recall, and F1-scores are all around 97-98%, indicating strong performance for males.\n    - **Female Classification**: Precision, recall, and F1-scores are slightly lower (around 93%) but still high, showing good performance for females as well.\n\n\n- **Variance Check**:\n    - **Training vs Test Accuracy**: Both accuracies are nearly identical (97.31% and 97.26%), meaning the model generalizes well without overfitting.\n\n\n- **Feature Importance**:\n    - **Income**: This is the most important feature, having the biggest impact on predictions.\n    - **Gender**: It's important but not as much as income.\n    - **Own_Housing and Own_Car**: These factors play a smaller role.\n    - **Num_Children**: This has almost no impact on the model's predictions.","metadata":{}},{"cell_type":"markdown","source":"## Saving the model","metadata":{}},{"cell_type":"code","source":"# Save the model to a .pkl file\nmodel_filename = 'optimized_logistic_regression_model.pkl'\nwith open(model_filename, 'wb') as file:\n    pickle.dump(best_logistic_regression, file)\n\nprint(f\"Model saved as {model_filename}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:35:41.238421Z","iopub.execute_input":"2024-10-25T16:35:41.239593Z","iopub.status.idle":"2024-10-25T16:35:41.248039Z","shell.execute_reply.started":"2024-10-25T16:35:41.239528Z","shell.execute_reply":"2024-10-25T16:35:41.246544Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model saved as optimized_logistic_regression_model.pkl\n","output_type":"stream"}]}]}